---
title: "KSPZV1 Baseline Multimodal ML revised"
author: "Leetah Senkpeil"
date: "2023-04-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, include=FALSE}
library(openxlsx)
library(xgboost)
library(dplyr)
library(tidyverse)
library(fgsea)
library(glmnet)
library(pROC)
library(aod)
library(caret)
library(limma)
library(caTools)
library(e1071)
library(robustbase)
library(Biobase)
library(doMC)
library(randomForest)
library(data.table)
datadir <- "/Users/tuantran/Library/CloudStorage/OneDrive-IndianaUniversity/Manuscripts/KSPZV1 Manuscript/kspzv1-systems-analysis/"
plotdir <- "/Users/tuantran/Library/CloudStorage/OneDrive-IndianaUniversity/Manuscripts/KSPZV1 Manuscript/JCI Resubmission April 2023/Figures JCI resubmission/"
```
```{r read in full dataset}
complete_baseline_eset <- read_rds(paste0(datadir, "KSPZV1 logCPM expression sets for visualization/PfSPZ_cpm_ExpressionSet_244x21815_AllGroups_bothBatches_0_rmBatchFX_06082021_TMT_logTRUE.rds"))
high_dose_features <- read_rds(paste0(datadir,"Leetah ML code/kspzv1_hidose_imputed_ml_datasetfull_042123.RDS"))
high_dose_baseline_eset <- complete_baseline_eset[,which(complete_baseline_eset$SAMPLEID %in% colnames(high_dose_features))]

if(all(colnames(high_dose_features) == colnames(high_dose_baseline_eset))){
  print("good to go!")
} else {
    print("please check to see if colnames match.")
  }
```

```{r split test and train}
#before splitting, we want to join the classification variable to the features data
high_dose_features_class <- pData(high_dose_baseline_eset) %>%
  dplyr::select(SAMPLEID, mal.atp.3) %>%
  mutate(class = factor(ifelse(mal.atp.3 == 1, "infected", "protected"))) %>% #convert to class factor with reference as "infected"
  dplyr::select(SAMPLEID, class) %>%
  left_join(., high_dose_features %>%
              t() %>%
              data.frame(check.names=FALSE) %>%
              rownames_to_column(var = "SAMPLEID"),
            by = "SAMPLEID") %>%
  column_to_rownames(var = "SAMPLEID")
#check dataframe
high_dose_features_class[1:6,1:6] 

# Splitting data in train and test data
set.seed(1234) #set seed for reproducibility
split <- sample.split(colnames(high_dose_baseline_eset), SplitRatio = 0.667)

train_eset <- high_dose_baseline_eset[,split==TRUE]
test_eset <- high_dose_baseline_eset[,split==FALSE]

train_features <- high_dose_features_class[split==TRUE,]
test_features <- high_dose_features_class[split==FALSE,]
```


```{r machine learning data setup}
model_df <- train_features %>%
  dplyr::select(-class)
  
outcome_df <- train_features %>%
  dplyr::select(class)

outcome <- outcome_df  %>%
  rownames_to_column(var = "sample_id") %>%
  deframe() %>%
  as.factor() #1 = infected, 0 = never_infected
#convert factor to numeric 
train_labels <- as.numeric(outcome)-1 #note that 0=infected (not protected) and 1 = protected

#check that outcome and model_df are in the same order
if(all(names(outcome) == rownames(model_df))){
  print("good to go!")
} else {
    print("please check to see if colnames match.")
}
```

### This part is derived from Leetah's original code

```{r ML rfe xgb}
#Looping to train best parameters
n_rfe_runs <- 3
number_feats_to_test <- 2:16 #extending to 16 
featlist <- list_param <- error_list <- rfe_run <- rfe_importance <- c() #making empty lists for loops
best_error_for_rfe_run <- best_iter <- best_nround <- best_seednumber <- best_param <- best_xgb_cv_eval_logs <- c() #set for every rfe run
registerDoMC(cores = 8) #use multi-core, 8 cores takes about 23% time required with single core
temp.seed <- sample.int(10000,1)[[1]] #set seed for reproducibility
 for (i in 1:n_rfe_runs) {
   #start_time <- Sys.time()
   rfe_res <- rfe(model_df, outcome, sizes = number_feats_to_test,
                  metric = 'Accuracy', maximize = TRUE, rfeControl = rfeControl(functions = rfFuncs)) #use RFE with randomForest for feature selection
   #end_time <- Sys.time()
   #rfe_eight_core_time <- end_time - start_time #used for evaluating performance
   rfe_run[[i]] <- rfe_res$results %>%
     filter(Variables %in% number_feats_to_test) %>%
     slice_max(order_by = Accuracy) #get most accurate combination
   top_features <- rownames(rfe_res$fit$importance[1:rfe_run[[i]]$Variables,]) #get top features
   rfe_run[[i]]$features <- paste(as.character(top_features), sep="' '", collapse="; ")
   rfe_importance[[i]] <- rfe_res$fit$importance[1:rfe_run[[i]]$Variables,] #save importance table for top features
   best_error <- Inf
   tested_seednumber <- temp.seed
   #run xgb on selected features with each round
   model_data <- xgb.DMatrix(data = as.matrix(model_df)[,top_features], label = train_labels, nthread = 8)
   #iterate to select best tuning parameters for given set of selected features
   for (iter in 1:1000) {
     param <- list(objective = 'binary:logistic',
                   max_depth = sample(2:15,1),
                   eta = runif(1, 0.01, 0.3),
                   gamma = runif(1, 0.0, 0.2),
                   subsample = runif(1, .6, .9),
                   colsample_bytree = runif(1, .5, .8),
                   min_child_weight = sample(1:10, 1),
                   lambda = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                   labmda_bias = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                   alpha = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1)
                   )
     cv.nround = 1000
     cv.nfold = 5
     seed.number = sample.int(10000,1)[[1]]
     set.seed(seed.number)
     mdcv <- xgb.cv(params = param,
                    data=model_data,
                    nthread=8,
                    verbose = TRUE,
                    nfold = cv.nfold,
                    nrounds = cv.nround,
                    early_stopping_rounds = 30,
                    maximize = FALSE,
                    metrics = "error")
     min_error <- min(mdcv$evaluation_log$test_error_mean) #get iteration with lowest test error
     print(glue::glue("current iter: ", iter, " best error: ", best_error, " using ", paste(as.character(top_features), sep="' '", collapse="; ")))
     
     if (min_error < best_error) {
       best_error <- min_error
       best_param[[i]]  <- data.frame(param,
                                      "best_iteration" = iter,
                                      "best_error" = best_error,
                                      "best_cv_nround" = mdcv$best_iteration,
                                      "best_seed_number" = seed.number)
       best_error_for_rfe_run[[i]] <- best_error
       best_iter[[i]]  <- iter
       best_nround[[i]]  <- mdcv$best_iteration
       best_seednumber[[i]]  <- seed.number
       best_xgb_cv_eval_logs[[i]] <- mdcv$evaluation_log[mdcv$evaluation_log$iter == mdcv$best_iteration,] #save evaluation log
     }
     }
   featlist[[i]] <- colnames(model_data)
   error_list[[i]] <- best_error
   save(best_param, best_nround, best_seednumber, best_xgb_cv_eval_logs, i, rfe_res, rfe_run, rfe_importance,
        file = paste0("/Users/tuantran/Library/CloudStorage/OneDrive-IndianaUniversity/Manuscripts/KSPZV1 Manuscript/ML results Tuan/",
                      "ML results TMT ", Sys.Date(), " ", n_rfe_runs ," runs.RData")
        )
   }
```

### Prepare training results

```{r prepare training results}
best_param_df <- bind_rows(best_param, .id = "rfe_run_number")

#this binds all features selected at each rfe run, then you can count up the number of appearances
rfe_importance_df <- rfe_importance %>%
  lapply(., as.data.frame) %>%
  lapply(., rownames_to_column, var = "feature") %>%
  bind_rows(., .id = "rfe_run_number")

rfe_appearances <- rfe_importance_df %>%
  group_by(feature) %>%
  summarize(appearances = n()) %>%
  arrange(desc(appearances)) %>%
  mutate(appearances_as_pct = 100*(appearances/200))

number_features_selected_per_rfe_run <- rfe_importance_df %>%
  group_by(rfe_run_number) %>%
  summarize(number_features_selected = n()) %>%
  arrange(desc(number_features_selected), as.integer(rfe_run_number))
```

## Use xgboost for feature selection and CV -- work in progress


```{r partition training and test}
train_dat <- t(train_features) %>%
  data.frame(check.names = FALSE) %>%
  t() %>%
  data.frame(check.names = FALSE) %>%
  mutate(class = factor(class)) %>%
  mutate_at(c(2:ncol(.)), as.numeric)
test_dat <- t(test_features) %>%
  data.frame(check.names = FALSE) %>%
  t() %>%
  data.frame(check.names = FALSE) %>%
  mutate(class = factor(class)) %>%
  mutate_at(c(2:ncol(.)), as.numeric)

#convert data frame to data table and preserve rownames
setDT(train_dat, keep.rownames = TRUE) 
train_dat_samplenames <- train_dat$rn
train_dat <- train_dat[,-1]

setDT(test_dat, keep.rownames = TRUE)
test_dat_samplenames <- test_dat$rn
test_dat <- test_dat[,-1]

#sanity check
if(all(colnames(train_dat) == colnames(train_features)) &
   all(train_dat_samplenames == rownames(train_features))){
  print("training set good to go!")
} else {
    print("please check to see if training samples and features match.")
}

if(all(colnames(test_dat) == colnames(test_features)) &
   all(test_dat_samplenames == rownames(test_features))){
  print("test set good to go!")
} else {
    print("please check to see if test samples and features match.")
}
```

```{r more checks, echo=FALSE, eval=FALSE}
#see https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/ for more data cleaning options

#check missing values 
# table(is.na(train_dat))
# sapply(train_dat, function(x) sum(is.na(x))/length(x))*100
# 
# table(is.na(test_dat))
# sapply(test_dat, function(x) sum(is.na(x))/length(x))*100
```

## Define class labels

```{r assign data and labels}
#using one hot encoding 
train_labels <- train_dat$class
test_labels <- test_dat$class
new_train <- model.matrix(~.+0,data = train_dat[,-c("class"),with=F]) 
new_test <- model.matrix(~.+0,data = test_dat[,-c("class"),with=F])


#convert factor to numeric 
train_labels <- as.numeric(train_labels)-1 #note that 0=infected (not protected) and 1 = protected
test_labels <- as.numeric(test_labels)-1 #note that 0=infected (not protected) and 1 = protected
```

## Prepare matrix

```{r prepare matrix}
dtrain <- xgb.DMatrix(data = new_train, label = train_labels)
dtest <- xgb.DMatrix(data = new_test, label = test_labels)
```

## set xgboost parameters

https://www.r-bloggers.com/2018/05/tuning-xgboost-in-r-part-i/

https://www.r-bloggers.com/2020/10/an-r-pipeline-for-xgboost-part-i/

## Use built-in xgb cross validation function to calcuate best nround and cv error

```{r xgbcv with hyperparameter tuning}
best_error <- Inf
model_data <- xgb.DMatrix(data = as.matrix(model_df), label = train_labels, nthread = 8)
for (iter in 1:2000) {
  param <- list(objective = 'binary:logistic',
                max_depth = sample(2:15,1),
                eta = runif(1, 0.01, 0.3),
                gamma = runif(1, 0.0, 0.2),
                subsample = runif(1, .6, .9),
                colsample_bytree = runif(1, .5, .8),
                min_child_weight = sample(1:10, 1),
                lambda = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                labmda_bias = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                alpha = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1)
                )
  cv.nround = 1000
  cv.nfold = 5
  seed.number = sample.int(10000,1)[[1]]
  set.seed(seed.number)
  bst.cv <- xgb.cv(params = param,
                 data=model_data,
                 nthread=8,
                 verbose = TRUE,
                 showsd = TRUE,
                 stratified = TRUE,
                 print_every_n = 20,
                 nfold = cv.nfold,
                 nrounds = cv.nround,
                 early_stopping_rounds = 30,
                 maximize = FALSE,
                 metrics = "error")

  min_error <- min(bst.cv$evaluation_log$test_error_mean) #get iteration with lowest test error
  print(glue::glue("current iter: ", iter, " best error: ", best_error))
  
  if (min_error < best_error) {
    best_error <- min_error
    best_param <- param
    best_param_df <- data.frame(param,
                                   "best_iteration" = iter,
                                   "best_error" = best_error,
                                   "best_cv_nround" = mdcv$best_iteration,
                                   "best_seed_number" = seed.number)
    best_iter  <- iter
    best_nround  <- mdcv$best_iteration
    best_seednumber  <- seed.number
    best_xgb_cv_eval_logs <- mdcv$evaluation_log[mdcv$evaluation_log$iter == mdcv$best_iteration,] #save evaluation log
  }
  }

save(best_param, best_nround, best_seednumber, best_xgb_cv_eval_logs, best_iter, best_error, bst.cv,
     file = paste0("/Users/tuantran/Library/CloudStorage/OneDrive-IndianaUniversity/Manuscripts/KSPZV1 Manuscript/ML results Tuan/",
                   "xgb cv only results TMT ", Sys.Date(), " ", n_rfe_runs ," runs.RData")
     )
```

## Plot validation error and training error against the number of rounds

```{r plot validation and training error per round}
cv.nround <- 2000
cv.nfold <- 5
bst.cv <- xgb.cv(params = best_param,
                 data=model_data,
                 nthread=8,
                 verbose = TRUE,
                 showsd = TRUE,
                 stratified = TRUE,
                 print_every_n = 5,
                 nfold = cv.nfold,
                 nrounds = cv.nround,
                 early_stopping_rounds = 30,
                 maximize = FALSE,
                 metrics = "error")

res_df <- data.frame(training_error = bst.cv$evaluation_log$train_error_mean, 
                     validation_error = bst.cv$evaluation_log$test_error_mean, # Don't confuse this with the test data set. 
                     iteration = bst.cv$evaluation_log$iter) %>%
  mutate(min = validation_error == min(validation_error))
best_nrounds <- res_df %>%
  filter(min) %>%
  pull(iteration)
res_df_longer <- pivot_longer(data = res_df, 
                              cols = c(training_error, validation_error), 
                              names_to = "error_type",
                              values_to = "error")

g <- ggplot(res_df_longer, aes(x = iteration)) +        # Look @ it overfit.
  geom_line(aes(y = error, group = error_type, colour = error_type)) +
  geom_vline(xintercept = best_nrounds, colour = "green") +
  #geom_label(aes(label = str_interp("${best_nrounds} iterations gives minimum validation error"), y = 0.2, x = best_nrounds, hjust = 0.1)) +
  labs(
    x = "nrounds",
    y = "Error",
    title = "Test & Train Errors",
    subtitle = str_interp("Note how the training error keeps decreasing after ${best_nrounds} iterations, but the validation error starts \ncreeping up. This is a sign of overfitting.")
  ) +
  scale_colour_discrete("Error Type: ")
g
```
## Test set accuracy

```{r test set accuracy}
#first default - model training
test_model_df <- test_features %>%
  dplyr::select(-class)
  
test_outcome_df <- test_features %>%
  dplyr::select(class)

test_outcome <- test_outcome_df  %>%
  rownames_to_column(var = "sample_id") %>%
  deframe() %>%
  as.factor() #1 = infected, 0 = never_infected
#convert factor to numeric 
test_labels <- as.numeric(test_outcome)-1 #note that 0=infected (not protected) and 1 = protected

train_data <- xgb.DMatrix(data = as.matrix(model_df), label = train_labels, nthread = 8)
test_data <- xgb.DMatrix(data = as.matrix(test_model_df), label = test_labels, nthread = 8)

xgb1 <- xgb.train(params = best_param,
                  data = train_data,
                  nrounds = 8,
                  watchlist = list(val=test_data,train=train_data),
                  print_every_n = 10,
                  early_stopping_rounds = 10,
                  maximize = FALSE,
                  eval_metric = "error")
#model prediction
xgbpred <- predict(xgb1,test_data)
xgbpred <- ifelse(xgbpred > 0.5,1,0)
```

## Plot accuracy as confusion matrix and look at importance plot

```{r confusion matrix and importance}
library(caret)
xgbpred <- factor(ifelse(xgbpred==1, "protected", "not protected"))
test_labels <- factor(ifelse(test_labels==1, "protected", "not protected"))
caret::confusionMatrix(xgbpred, test_labels)

#view variable importance plot
mat <- xgb.importance (feature_names = colnames(train_data),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20]) 
```